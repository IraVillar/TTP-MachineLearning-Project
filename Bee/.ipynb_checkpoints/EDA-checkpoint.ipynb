{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 1. Understanding the Dataset </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> IMPORT </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'train.csv' does not exist: b'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e65deb0e5e05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#import dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'train.csv' does not exist: b'train.csv'"
     ]
    }
   ],
   "source": [
    "#import dataset\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> UNDERSTANDING THE DATA </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check the columns of train dataset\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#structure of train dataset\n",
    "print('original rows:', train.shape[0], 'original columns:', train.shape[1])\n",
    "\n",
    "#structure of test dataset\n",
    "print('test rows:', test.shape[0], 'test columns:', test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training data information\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#testing data information\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#types of variables \n",
    "np.unique(train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables with float64\n",
    "train.select_dtypes(include = ['float64']).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables with integer\n",
    "train.select_dtypes(include = ['int64']).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Variables with object\n",
    "train.select_dtypes(include = ['object']).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Variables </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical Variables\n",
    "numerics = ['int64', 'float64']\n",
    "numeric_train  = train.select_dtypes(include = numerics)  \n",
    "numeric_train = numeric_train.drop(columns = ['BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath','GarageYrBlt', 'MoSold', 'YrSold', 'YearRemodAdd', 'OverallQual',\n",
    "                                             'OverallCond', 'YearBuilt','MSSubClass'])\n",
    "numeric_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor Variables\n",
    "factors = ['MSSubClass', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'MoSold', 'YrSold']\n",
    "train_factors = train[factors]\n",
    "train_factors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Variables: Turned into Dummies by Owner\n",
    "categories = ['BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath']\n",
    "train_categories = train[categories]\n",
    "train_categories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Variables \n",
    "categori = ['object']\n",
    "catego = train.select_dtypes(include= categori)\n",
    "catego.head()\n",
    "categorical_train = pd.concat([catego, train_categories], axis=1, sort=False)\n",
    "categorical_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> UNDERSTAND THE SALEPRICE </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#summary on SalePrice(target variable) from train dataset\n",
    "train['SalePrice'].describe()\n",
    "#All prices are greater than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How expensive are houses?\n",
    "import matplotlib.pyplot as plt\n",
    "print('The cheapest house sold for ${:,.0f} and the most expensive for ${:,.0f}'.format(\n",
    "    train.SalePrice.min(), train.SalePrice.max()))\n",
    "print('The average sales price is ${:,.0f}, while median is ${:,.0f}'.format(\n",
    "    train.SalePrice.mean(), train.SalePrice.median()))\n",
    "train.SalePrice.hist(bins=75, rwidth=.8, figsize=(14,4))\n",
    "plt.title('How expensive are houses?')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When were the houses built?\n",
    "print('Oldest house built in {}. Newest house built in {}.'.format(\n",
    "    train.YearBuilt.min(), train.YearBuilt.max()))\n",
    "train.YearBuilt.hist(bins=14, rwidth=.9, figsize=(12,4))\n",
    "plt.title('When were the houses built?')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales Price\n",
    "print('Skew: {:.3f} | Kurtosis: {:.3f}'.format(train.SalePrice.skew(), train.SalePrice.kurtosis()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram of SalePrice to see the distribution \n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(14,4))\n",
    "sns.distplot(train['SalePrice'], ax = ax1)\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('SalePrice Distribution')\n",
    "#QQ-plot\n",
    "stats.probplot(train['SalePrice'], plot=plt)\n",
    "plt.show()\n",
    "#this is right skewed (violating assumptions of linear regression) so we will need to normalize. \n",
    "#-> power transformation(rightskew -> power >1) or log transformation or box cox?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> EDA </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a scatter plot with dist plot for all numeric variables in the train data \n",
    "#by Sale Price\n",
    "sns.jointplot(x=\"LotFrontage\", y=\"SalePrice\", data=numeric_train, kind = 'reg')\n",
    "sns.jointplot(x=\"LotArea\", y=\"SalePrice\", data=numeric_train, kind = 'reg')\n",
    "sns.jointplot(x=\"MasVnrArea\", y=\"SalePrice\", data=numeric_train, kind = 'reg')\n",
    "sns.jointplot(x=\"BsmtFinSF1\", y=\"SalePrice\", data=numeric_train, kind = 'reg')\n",
    "sns.jointplot(x=\"1stFlrSF\", y=\"SalePrice\", data=numeric_train, kind = 'reg')\n",
    "sns.jointplot(x=\"2ndFlrSF\", y=\"SalePrice\", data=numeric_train, kind = 'reg')\n",
    "sns.jointplot(x='LowQualFinSF', y=\"SalePrice\", data=numeric_train, kind = 'reg')\n",
    "sns.jointplot(x=\"GrLivArea\", y=\"SalePrice\", data=numeric_train, kind = 'reg')\n",
    "sns.jointplot(x=\"BedroomAbvGr\", y=\"SalePrice\", data=numeric_train, kind = 'reg')\n",
    "sns.jointplot(x=\"KitchenAbvGr\", y=\"SalePrice\", data=numeric_train, kind = 'reg')\n",
    "sns.jointplot(x=\"TotRmsAbvGrd\", y=\"SalePrice\", data=numeric_train, kind = 'reg')\n",
    "sns.jointplot(x=\"GarageCars\", y=\"SalePrice\", data=numeric_train, kind = 'reg')\n",
    "sns.jointplot(x=\"GarageArea\", y=\"SalePrice\", data=numeric_train, kind = 'reg')\n",
    "sns.jointplot(x=\"WoodDeckSF\", y=\"SalePrice\", data=numeric_train, kind = 'reg')\n",
    "sns.jointplot(x=\"OpenPorchSF\", y=\"SalePrice\", data=numeric_train, kind = 'reg')\n",
    "sns.jointplot(x=\"EnclosedPorch\", y=\"SalePrice\", data=numeric_train, kind = 'reg')\n",
    "sns.jointplot(x=\"3SsnPorch\", y=\"SalePrice\", data=numeric_train, kind = 'reg')\n",
    "sns.jointplot(x=\"ScreenPorch\", y=\"SalePrice\", data=numeric_train, kind = 'reg')\n",
    "sns.jointplot(x=\"MiscVal\", y=\"SalePrice\", data=numeric_train, kind = 'reg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distribution Plots for Numerical Features\n",
    "# Grid of distribution plots of all numerical features\n",
    "f = pd.melt(numeric_train, value_vars=sorted(numeric_train))\n",
    "g = sns.FacetGrid(f, col='variable', col_wrap=4, sharex=False, sharey=False)\n",
    "g = g.map(sns.distplot, 'value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Numeric variables correlation\n",
    "numcor = train.corr()\n",
    "colormap = plt.cm.RdBu\n",
    "f, ax = plt.subplots(figsize = (9,8))\n",
    "sns.heatmap(numcor, ax=ax, cmap = colormap, linewidths = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = numcor.unstack()\n",
    "s[(abs(s)>0.6) & (abs(s) < 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Variables\n",
    "categorical_train.columns\n",
    "f = pd.melt(categorical_train, value_vars=sorted(categorical_train))\n",
    "g = sns.FacetGrid(f, col='variable', col_wrap=4, sharex=False, sharey=False)\n",
    "plt.xticks(rotation='vertical')\n",
    "g = g.map(sns.countplot, 'value')\n",
    "[plt.setp(ax.get_xticklabels(), rotation=60) for ax in g.axes.flat]\n",
    "g.fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plot for Categorical Features\n",
    "f = pd.melt(train, id_vars=['SalePrice'], value_vars=sorted(categorical_train))\n",
    "g = sns.FacetGrid(f, col='variable', col_wrap=3, sharex=False, sharey=False, size=4)\n",
    "g = g.map(sns.boxplot, 'value', 'SalePrice')\n",
    "[plt.setp(ax.get_xticklabels(), rotation=90) for ax in g.axes.flat]\n",
    "g.fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphing Factors\n",
    "f = pd.melt(train_factors, value_vars=sorted(train_factors))\n",
    "g = sns.FacetGrid(f, col='variable', col_wrap=4, sharex=False, sharey=False)\n",
    "plt.xticks(rotation='vertical')\n",
    "g = g.map(sns.countplot, 'value')\n",
    "[plt.setp(ax.get_xticklabels(), rotation=60) for ax in g.axes.flat]\n",
    "g.fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 2. Data Cleaning </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Missing </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values Data Frame: Train \n",
    "missing = train.isna().sum()\n",
    "missing = missing[missing>0]\n",
    "missing_percent = missing/train.shape[0] * 100\n",
    "train_missing = pd.DataFrame([missing, missing_percent], index = ['total', 'missing percent']).T\n",
    "train_missing.sort_values(['missing percent'], ascending = [False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Missing Values Data Frame: Test\n",
    "missing_test = test.isna().sum()\n",
    "missing_test = missing_test[missing_test>0]\n",
    "missingtest_percent = missing_test/test.shape[0] * 100\n",
    "test_missing = pd.DataFrame([missing_test, missingtest_percent], index = ['total', 'missing percent']).T\n",
    "test_missing.sort_values(['missing percent'], ascending = [False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some that has missing only in train dataset and only in test dataset.\n",
    "# first drop the SalePrice column of train dataset and\n",
    "# then we will combine two dataset and then clean it. \n",
    "trainX = train.drop('SalePrice', axis =1)     #1460 rows with 80 columns\n",
    "testX = test                                  #1459 rows with 80 columns\n",
    "test_train = pd.concat([trainX, testX], keys=['train', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the test_train dataset\n",
    "test_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns with so many missing values. \n",
    "test_train = test_train.drop(columns= ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'Id'])\n",
    "#not dropping poolarea since we can use that to assume that existing value means \n",
    "#there is a pool and if data is missing, it might be an indication that there is no pool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the test_train dataset\n",
    "test_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the original Missing Values Data Frame: Test_Train \n",
    "missing = test_train.isna().sum()\n",
    "missing = missing[missing>0]\n",
    "missing_percent = missing/test_train.shape[0] * 100\n",
    "test_train_missing = pd.DataFrame([missing, missing_percent], index = ['total', 'missing percent']).T\n",
    "test_train_missing.sort_values(['missing percent'], ascending = [False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Imputation: Filling Missing Values \n",
    "test_train.loc[:, \"BedroomAbvGr\"] = test_train.loc[:, \"BedroomAbvGr\"].fillna(0)\n",
    "test_train.loc[:, \"BsmtQual\"] = test_train.loc[:, \"BsmtQual\"].fillna(\"No\")\n",
    "test_train.loc[:, \"BsmtCond\"] = test_train.loc[:, \"BsmtCond\"].fillna(\"No\")\n",
    "test_train.loc[:, \"BsmtExposure\"] = test_train.loc[:, \"BsmtExposure\"].fillna(\"No\")\n",
    "test_train.loc[:, \"BsmtFinType1\"] = test_train.loc[:, \"BsmtFinType1\"].fillna(\"No\")\n",
    "test_train.loc[:, \"BsmtFinType2\"] = test_train.loc[:, \"BsmtFinType2\"].fillna(\"No\")\n",
    "test_train.loc[:, \"BsmtFullBath\"] = test_train.loc[:, \"BsmtFullBath\"].fillna(0)\n",
    "test_train.loc[:, \"BsmtHalfBath\"] = test_train.loc[:, \"BsmtHalfBath\"].fillna(0)\n",
    "test_train.loc[:, \"BsmtUnfSF\"] = test_train.loc[:, \"BsmtUnfSF\"].fillna(0)\n",
    "test_train.loc[:, \"CentralAir\"] = test_train.loc[:, \"CentralAir\"].fillna(\"N\")\n",
    "test_train.loc[:, \"Condition1\"] = test_train.loc[:, \"Condition1\"].fillna(\"Norm\")\n",
    "test_train.loc[:, \"Condition2\"] = test_train.loc[:, \"Condition2\"].fillna(\"Norm\")\n",
    "test_train.loc[:, \"EnclosedPorch\"] = test_train.loc[:, \"EnclosedPorch\"].fillna(0)\n",
    "test_train.loc[:, \"ExterCond\"] = test_train.loc[:, \"ExterCond\"].fillna(\"TA\")\n",
    "test_train.loc[:, \"ExterQual\"] = test_train.loc[:, \"ExterQual\"].fillna(\"TA\")\n",
    "test_train.loc[:, \"FireplaceQu\"] = test_train.loc[:, \"FireplaceQu\"].fillna(\"No\")\n",
    "test_train.loc[:, \"Fireplaces\"] = test_train.loc[:, \"Fireplaces\"].fillna(0)\n",
    "test_train.loc[:, \"Functional\"] = test_train.loc[:, \"Functional\"].fillna(\"Typ\")\n",
    "test_train.loc[:, \"GarageType\"] = test_train.loc[:, \"GarageType\"].fillna(\"No\")\n",
    "test_train.loc[:, \"GarageFinish\"] = test_train.loc[:, \"GarageFinish\"].fillna(\"No\")\n",
    "test_train.loc[:, \"GarageQual\"] = test_train.loc[:, \"GarageQual\"].fillna(\"No\")\n",
    "test_train.loc[:, \"GarageCond\"] = test_train.loc[:, \"GarageCond\"].fillna(\"No\")\n",
    "test_train.loc[:, \"GarageArea\"] = test_train.loc[:, \"GarageArea\"].fillna(0)\n",
    "test_train.loc[:, \"GarageCars\"] = test_train.loc[:, \"GarageCars\"].fillna(0)\n",
    "test_train.loc[:, \"HalfBath\"] = test_train.loc[:, \"HalfBath\"].fillna(0)\n",
    "test_train.loc[:, \"HeatingQC\"] = test_train.loc[:, \"HeatingQC\"].fillna(\"TA\")\n",
    "test_train.loc[:, \"KitchenAbvGr\"] = test_train.loc[:, \"KitchenAbvGr\"].fillna(0)\n",
    "test_train.loc[:, \"KitchenQual\"] = test_train.loc[:, \"KitchenQual\"].fillna(\"TA\")\n",
    "test_train.loc[:, \"LotFrontage\"] = test_train.loc[:, \"LotFrontage\"].fillna(0)\n",
    "test_train.loc[:, \"LotShape\"] = test_train.loc[:, \"LotShape\"].fillna(\"Reg\")\n",
    "test_train.loc[:, \"MasVnrType\"] = test_train.loc[:, \"MasVnrType\"].fillna(\"None\")\n",
    "test_train.loc[:, \"MasVnrArea\"] = test_train.loc[:, \"MasVnrArea\"].fillna(0)\n",
    "test_train.loc[:, \"MiscVal\"] = test_train.loc[:, \"MiscVal\"].fillna(0)\n",
    "test_train.loc[:, \"OpenPorchSF\"] = test_train.loc[:, \"OpenPorchSF\"].fillna(0)\n",
    "test_train.loc[:, \"PavedDrive\"] = test_train.loc[:, \"PavedDrive\"].fillna(\"N\")\n",
    "test_train.loc[:, \"SaleCondition\"] = test_train.loc[:, \"SaleCondition\"].fillna(\"Normal\")\n",
    "test_train.loc[:, \"ScreenPorch\"] = test_train.loc[:, \"ScreenPorch\"].fillna(0)\n",
    "test_train.loc[:, \"TotRmsAbvGrd\"] = test_train.loc[:, \"TotRmsAbvGrd\"].fillna(0)\n",
    "test_train.loc[:, \"Utilities\"] = test_train.loc[:, \"Utilities\"].fillna(\"AllPub\")\n",
    "test_train.loc[:, \"WoodDeckSF\"] = test_train.loc[:, \"WoodDeckSF\"].fillna(0)\n",
    "test_train.loc[:, \"Exterior1st\"] = test_train.loc[:, \"Exterior1st\"].fillna(\"No\")\n",
    "test_train.loc[:, \"Exterior2nd\"] = test_train.loc[:, \"Exterior2nd\"].fillna(\"No\")\n",
    "test_train.loc[:, \"BsmtFinSF1\"] = test_train.loc[:, \"BsmtFinSF1\"].fillna(0)\n",
    "test_train.loc[:, \"BsmtFinSF2\"] = test_train.loc[:, \"BsmtFinSF2\"].fillna(0)\n",
    "test_train.loc[:, \"TotalBsmtSF\"] = test_train.loc[:, \"TotalBsmtSF\"].fillna(0)\n",
    "test_train.loc[:, \"Electrical\"] = test_train.loc[:, \"Electrical\"].fillna(\"Electrical\")\n",
    "test_train.loc[:, \"SaleType\"] = test_train.loc[:, \"SaleType\"].fillna(\"WD\")\n",
    "test_train.loc[:, \"GarageYrBlt\"] = test_train.loc[:, \"GarageYrBlt\"].fillna(\"None\")\n",
    "test_train.loc[:, \"PoolArea\"] = test_train.loc[:, \"PoolArea\"].fillna(\"0\")\n",
    "test_train.loc[:, \"MSZoning\"] = test_train.loc[:, \"MSZoning\"].fillna(\"RL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Final Missing Values Data Frame: Test_Train \n",
    "missing = test_train.isna().sum()\n",
    "missing = missing[missing>0]\n",
    "missing_percent = missing/test_train.shape[0] * 100\n",
    "test_train_missing = pd.DataFrame([missing, missing_percent], index = ['total', 'missing percent']).T\n",
    "test_train_missing.sort_values(['missing percent'], ascending = [False])\n",
    "#nothing missing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create a variable for Total SF\n",
    "#Combine all Bsmt + 1st + 2nd fl, does not distinguish between quality\n",
    "test_train['TotalSF'] = test_train['TotalBsmtSF'] + test_train['1stFlrSF'] + test_train['2ndFlrSF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a variable for Total SF\n",
    "#Combine all Bsmt + 1st + 2nd fl, does not distinguish between quality\n",
    "test_train['TotalSF'] = test_train['TotalBsmtSF'] + test_train['1stFlrSF'] + test_train['2ndFlrSF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Variable For Total Bath\n",
    "#Half Baths are multiplied by 0.5 and Full are added as a whole\n",
    "test_train['TotalBath'] = test_train['BsmtFullBath'] + test_train['FullBath'] + 0.5* test_train['BsmtHalfBath'] + 0.5 * test_train['HalfBath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Variable For Total Porch SF\n",
    "#We do not distinguish between the variables\n",
    "test_train['TotalPorchSF'] = test_train['WoodDeckSF'] + test_train['OpenPorchSF'] + test_train['EnclosedPorch']+ test_train['3SsnPorch']+ test_train['ScreenPorch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dummy variable for finished bsmt\n",
    "#not distinguishing between finish quality for basement only if the basement is unfinished\n",
    "test_train['BsmtFin']= (test_train['BsmtFinType1'] != 'Unf')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check the final test_train after imputation and dummification\n",
    "test_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final test and train dataset\n",
    "final_train = test_train.iloc[:1460,:]\n",
    "final_test = test_train.iloc[1460:,:]\n",
    "print('train', final_train.shape, 'final_test', final_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#created SalePrice df that just includes SalePrice. \n",
    "SalePrice = train.iloc[:,-1:]\n",
    "SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index\n",
    "final_train = final_train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put back the SalePrice to train dataset\n",
    "final_train['SalePrice'] = SalePrice['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the final_train dataset\n",
    "final_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting final cleaned train dataset and cleaned \n",
    "final_train.to_csv('cleanedtrain.csv')\n",
    "final_test.to_csv('cleanedtest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
